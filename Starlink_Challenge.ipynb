{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starlink_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCptGSCd/HcgiAFuRnCIHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tajara/starlink_api_ingestion/blob/main/Starlink_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blue Onion Labs Take Home Test\n",
        "Hey! We are stoked that you are interested in joining the team at Blue Onion Labs.\n",
        "\n",
        "We have crafted the following test to see how you approach pulling and manipulating of data. We want to get a general idea of how you approach some common types of problems that we encounter here at Blue Onion (we are really proficient at integrations!)\n",
        "\n",
        "Background\n",
        "spacexdata.com provides an API to query attributes about SpaceX launches (https://github.com/r-spacex/SpaceX-API/tree/master/docs#rspacex-api-docs). For this exercise we are going to be working with one resource in particular:\n",
        "\n",
        "The Starlink Schema\n",
        "For this exercise, no need to pull directly from the API as we have a pull of historical data here in this repo in the starlink_historical_data.json\n",
        "\n",
        "The Problem:\n",
        "We want to be achieve a few goals:\n",
        "\n",
        "To import the SpaceX Satellite data as a time series into a database\n",
        "To be able to query the data to determine the last known latitude/longitude of the satellite for a given time\n",
        "The Task (Part 1):\n",
        "Stand up your favorite kind of database (and ideally it would be in a form that would be runnable by us, via something like docker-compose).\n",
        "\n",
        "The Task (Part 2):\n",
        "Write a script (in whatever language that you prefer, though Ruby, Python, or Javascript would be ideal for us) to import the relevant fields in starlink_historical_data.json as a time series. The relevant fields are: - spaceTrack.creation_date (represents the time that the lat/lon records were recorded) - longitude - latitude - id (this is the starlink satellite id) Again, the goal is that we want to be able to query the database for the last known position for a given starlink satellite. Don't hesitate to use any tools/tricks you know to load data quickly and easily!\n",
        "\n",
        "The Task (Part 3):\n",
        "Write a query to fetch the last known position of a satellite (by id), given a time T. Include this query in your README or somewhere in the project submission\n",
        "\n",
        "Bonus Task (Part 4):\n",
        "Write some logic (via a combination of query + application logic, most likely) to fetch from the database the closest satellite at a given time T, and a given a position on a globe as a (latitude, longitude) coordinate.\n",
        "\n",
        "No need to derive any fancy match for distances for a point on the globe to a position above the earth. You can just use the Haversine formula. Example libraries to help here:\n",
        "\n",
        "For Python: https://github.com/mapado/haversine\n",
        "\n",
        "For Ruby: https://github.com/kristianmandrup/haversine\n",
        "\n",
        "How to Submit\n",
        "Run through it one last time to make sure it works!\n",
        "Push the code up to your repo one last time (or save your working directory to a 'zip')\n",
        "Reach out to us with your solution\n",
        "Questions\n",
        "If you have any questions at all during the challenge do not hesitate to reach out! Whether it be a question about the requirements, submitting, anything, just send us a note!"
      ],
      "metadata": {
        "id": "rtCDec4t9LdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cqX7fACV9Kr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pyspark installation\n",
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Cyixiwf-nN",
        "outputId": "ed8ec82c-7e85-47ad-a731-fa430f3f268f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SqlAlchemy Installation\n",
        "!pip install sqlalchemy"
      ],
      "metadata": {
        "id": "vytTs1rOYEKh",
        "outputId": "44861c85-2a74-429e-e43e-6539956ce592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (1.4.36)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy) (3.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading sql extension on sqlite\n",
        "%load_ext sql"
      ],
      "metadata": {
        "id": "QI1gVJgDEV2O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lib Imports\n",
        "from sqlalchemy import create_engine\n",
        "import sqlite3\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#spark setup\n",
        "spark = SparkSession.builder.appName('starlink_data_ingestion').getOrCreate()\n"
      ],
      "metadata": {
        "id": "_84sRzf1YaDm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTIONS 1 AND 2 - DATA INGESTION TO DATABASE"
      ],
      "metadata": {
        "id": "4b6Gq8fzCpA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATING DATA OUTPUT TABLE starlink ON test.db\n",
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('test.db')\n",
        "print(\"Opened database successfully\");\n",
        "\n",
        "conn.execute('''\n",
        "CREATE TABLE  IF NOT EXISTS starlink (\n",
        "  'index' integer\n",
        "  ,height_km decimal(5,18) \n",
        "  ,id varchar(200) \n",
        "  ,latitude decimal(5,18)\n",
        "  ,launch varchar(200) \n",
        "  ,longitude decimal(5,18)\n",
        "  ,velocity_kms decimal(5,18) \n",
        "  ,version varchar(200) \n",
        "  ,inclination varchar(200) \n",
        "  ,arg_of_pericenter varchar(200) \n",
        "  ,mean_motion varchar(200) \n",
        "  ,decay_date varchar(200) \n",
        "  ,periapsis varchar(200) \n",
        "  ,ra_of_asc_node varchar(200) \n",
        "  ,ephemeris_type varchar(200) \n",
        "  ,site varchar(200) \n",
        "  ,rcs_size varchar(200) \n",
        "  ,norad_cat_id varchar(200) \n",
        "  ,mean_motion_ddot varchar(200) \n",
        "  ,apoapsis varchar(200) \n",
        "  ,comment varchar(200) \n",
        "  ,mean_anomaly varchar(200) \n",
        "  ,tle_line0 varchar(200) \n",
        "  ,mean_element_theory varchar(200) \n",
        "  ,object_name varchar(200) \n",
        "  ,mean_motion_dot varchar(200) \n",
        "  ,eccentricity varchar(200) \n",
        "  ,classification_type varchar(200) \n",
        "  ,ccsds_omm_vers varchar(200) \n",
        "  ,time_system varchar(200) \n",
        "  ,element_set_no varchar(200) \n",
        "  ,ref_frame varchar(200) \n",
        "  ,country_code varchar(200) \n",
        "  ,epoch varchar(200) \n",
        "  ,period varchar(200) \n",
        "  ,tle_line1 varchar(200) \n",
        "  ,file varchar(200) \n",
        "  ,bstar varchar(200) \n",
        "  ,tle_line2 varchar(200) \n",
        "  ,rev_at_epoch varchar(200) \n",
        "  ,decayed varchar(200) \n",
        "  ,semimajor_axis varchar(200) \n",
        "  ,object_type varchar(200) \n",
        "  ,object_id varchar(200) \n",
        "  ,gp_id varchar(200) \n",
        "  ,creation_date varchar(200) \n",
        "  ,launch_date varchar(200) \n",
        "  ,center_name varchar(200) \n",
        "  ,originator varchar(200)\n",
        ");\n",
        "''')\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "print(\"Table created successfully\");\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "nauV2nZWV9rK",
        "outputId": "bf104b44-c4ce-4078-890b-9b08f13f16ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opened database successfully\n",
            "Table created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BVknl7c_CKNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3163b6-586c-4289-c1cd-454d00e60f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "list created with API data\n",
            "Pyspark Dataframe Created\n"
          ]
        }
      ],
      "source": [
        "#Class responsible to access the starlink api and append the results in a list (lst)\n",
        "\n",
        "class DataIngestion():\n",
        "      \n",
        "    def api_request(self):\n",
        "      #Access the Api and return the json row\n",
        "        loaded_data = requests.get(\n",
        "            f'https://api.spacexdata.com/v4/starlink')\n",
        "        if loaded_data.status_code == 200:\n",
        "            return loaded_data.json()\n",
        "        else:\n",
        "            return loaded_data.status_code\n",
        "\n",
        "    def data_ingestion(self):\n",
        "      #Loop into the data available appending it in a list\n",
        "        lst_json = []\n",
        "\n",
        "        api_data = self.api_request()\n",
        "        if type(api_data) is not int:\n",
        "            for i in range(len(api_data)):\n",
        "               lst_json.append(api_data[i])\n",
        "        else:\n",
        "            print(api_data)\n",
        "        return lst_json\n",
        "\n",
        "path = DataIngestion()\n",
        "json_list = path.data_ingestion()\n",
        "print('list created with API data')\n",
        "\n",
        "#Pyspark Dataframe creation from the json list\n",
        "df = spark.createDataFrame(json_list)\n",
        "print('Pyspark Dataframe Created')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DATA TRANSFORMATION\n",
        "\n",
        "class DataTransformation():\n",
        "\n",
        "  def dataframeCreation(self,df):\n",
        "    #Pyspark Dataframe creation from the json list\n",
        "    df = spark.createDataFrame(json_list)\n",
        "    return df\n",
        "\n",
        "  def explodeSpaceTrack(self,df):\n",
        "    #The data where structured in columns on pyspark dataframe, but the column spaceTrack at this point it`s not splitted below there`s an example of a register from the column\n",
        "    #{TIME_SYSTEM -> UTC, TLE_LINE1 -> 1 44244U 19029K   20287.12291165  .47180237  12426-4  22139-2 0  9995, TLE_LINE0 -> 0 STARLINK-30, TLE_LINE2 -> 2 44244  52.9708 332.0356 0003711 120.7278 242.0157 16.43170483 77756, RA_OF_ASC_NODE -> 332.0356, ELEMENT_SET_NO -> 999, MEAN_ANOMALY -> 242.0157, CENTER_NAME -> EARTH, APOAPSIS -> 159.809, DECAYED -> 1, OBJECT_TYPE -> PAYLOAD, COMMENT -> GENERATED VIA SPACE-TRACK.ORG API, MEAN_MOTION_DOT -> 0.47180237, OBJECT_ID -> 2019-029K, MEAN_MOTION -> 16.43170483, CLASSIFICATION_TYPE -> U, INCLINATION -> 52.9708, EPOCH -> 2020-10-13T02:56:59.566560, FILE -> 2850561, PERIAPSIS -> 154.958, BSTAR -> 0.0022139, ORIGINATOR -> 18 SPCS, MEAN_MOTION_DDOT -> 1.2426E-5, SITE -> AFETR, DECAY_DATE -> 2020-10-13, LAUNCH_DATE -> 2019-05-24, NORAD_CAT_ID -> 44244, CREATION_DATE -> 2020-10-13T04:16:08, REF_FRAME -> TEME, MEAN_ELEMENT_THEORY -> SGP4, SEMIMAJOR_AXIS -> 6535.519, EPHEMERIS_TYPE -> 0, ARG_OF_PERICENTER -> 120.7278, CCSDS_OMM_VERS -> 2.0, PERIOD -> 87.635, ECCENTRICITY -> 3.711E-4, OBJECT_NAME -> STARLINK-30, RCS_SIZE -> LARGE, REV_AT_EPOCH -> 7775, GP_ID -> 163365918, COUNTRY_CODE -> US}\n",
        "    #So, the code on this cell is responsible to persist the columns defined on de variable: col and split the registers on the column spaceTrack in new columns\n",
        "    cols = ['height_km','id','latitude','launch','longitude','velocity_kms','version']\n",
        "\n",
        "    keys_df = df.select(\n",
        "        explode(map_keys(df.spaceTrack))).distinct()\n",
        "    keys = list(map(lambda row: row[0], keys_df.collect()))\n",
        "    key_cols = list(map(lambda f: df.spaceTrack.getItem(f).alias(str(f)), keys))\n",
        "    final_cols = cols + key_cols\n",
        "\n",
        "    print('spaceTrack field splitted')\n",
        "    return df.select(final_cols)\n",
        "  \n",
        "  def columnsToLower(self,df):\n",
        "    #The keys on the column spaceTrack are on Higher case, so the script below turn them all the columns on dataframe into lower case \n",
        "    for col in df.columns:\n",
        "      df = df.withColumnRenamed(col, col.lower())\n",
        "\n",
        "    print('columns renamed')  \n",
        "    return df\n",
        "  \n",
        "  def castToPandas(self,df):\n",
        "    #A Pandas Dataframe is created from a pyspark dataframe\n",
        "    df1 = df.toPandas() \n",
        "    print('pandas dataframe created')\n",
        "\n",
        "    return df1\n",
        "\n",
        "transform = DataTransformation()\n",
        "#There`s no need of memory allocation with multiple dataframes, so the code below overwrites the previously dataframe \n",
        "df = transform.dataframeCreation(df)\n",
        "df = transform.explodeSpaceTrack(df)\n",
        "df = transform.columnsToLower(df)\n",
        "pd_df = transform.castToPandas(df)\n"
      ],
      "metadata": {
        "id": "MkYo_u_zp2Yu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a917d3a-d5a5-45c8-890c-ffb4f6b65695"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaceTrack field splitted\n",
            "columns renamed\n",
            "pandas dataframe created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoad():\n",
        "\n",
        "  def createEngine():\n",
        "    #The native database (Sqlite) from google colab is being used on this challenge so, in this cell the engine with the database test.db is created\n",
        "    engine = create_engine('sqlite:///test.db', echo=False)\n",
        "    print('engine created')\n",
        "    return engine\n",
        "\n",
        "  def insertSqliteDB(pd_df,engine):\n",
        "    #Insert on table starlink from the pandas dataframe with append mode is executed\n",
        "    pd_df.to_sql('starlink', con=engine, if_exists='append')\n",
        "    print('data insertion completed')\n",
        "    return\n",
        "\n",
        "load = DataLoad()\n",
        "engine = DataLoad.createEngine()\n",
        "DataLoad.insertSqliteDB(pd_df,engine)\n"
      ],
      "metadata": {
        "id": "151kXq3NSbfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5357d9e-e558-4f60-bbeb-6df86231f9d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "engine created\n",
            "data insertion completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Task (Part 3): Write a query to fetch the last known position of a satellite (by id), given a time T. Include this query in your README or somewhere in the project submission**"
      ],
      "metadata": {
        "id": "2T2oDZSlC1ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function responsible to print the results from the question 3\n",
        "\n",
        "def satellite_position(id):\n",
        " \n",
        "  conn = sqlite3.connect('test.db')\n",
        "\n",
        "  cursor = conn.execute(''' SELECT \n",
        "                            id,\n",
        "\n",
        "                            datetime(epoch),\n",
        "                            CASE \n",
        "                              WHEN latitude IS NULL THEN 'unavailable'\n",
        "                            ELSE\n",
        "                                latitude\n",
        "                            END \n",
        "                                latitude,\n",
        "                            CASE \n",
        "                              WHEN longitude IS NULL THEN 'unavailable'\n",
        "                            ELSE\n",
        "                                longitude\n",
        "                            END \n",
        "                                longitude\n",
        "\n",
        "                            FROM \n",
        "                              starlink\n",
        "                            WHERE\n",
        "                              id = ?\n",
        "                            ORDER BY\n",
        "                              datetime(epoch) desc\n",
        "                            LIMIT \n",
        "                              1\n",
        "                            \n",
        "                          ;''',[id])\n",
        "\n",
        "  for row in cursor:\n",
        "    print(row)\n",
        "  conn.close()\n",
        "\n",
        "  return \n",
        "  "
      ],
      "metadata": {
        "id": "ATeJRlWIGD0e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION 3 TEST - PROVIDE DE SATTELITE ID TO KNOW THE LAST POSITION IN LATITUDE AND LONGITUDE\n",
        "#Edit the variable id with an id string, there`s some examples below\n",
        "#5eed7714096e59000698562b\n",
        "#5eed7714096e5900069856b0\n",
        "#5eed7715096e59000698573d\n",
        "#5eed7716096e5900069857ec\n",
        "#5fc7ec8be4130000069e2c02\n",
        "#5eed7714096e59000698567d\n",
        "#5eed7714096e590006985680\n",
        "#5eed7715096e59000698575d\n",
        "#5eed7715096e590006985760\n",
        "#5eed7716096e5900069857d6\n",
        "\n",
        "\n",
        "id = '5eed7714096e590006985648'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "schema = \"('id','epoch','latitude','longitude')\"\n",
        "print('Question 3 resolution')\n",
        "print(schema)\n",
        "satellite_position(id)\n"
      ],
      "metadata": {
        "id": "FPDWpSXr22bg",
        "outputId": "5769c5e1-b272-4810-9d60-cc8195ea41cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3 resolution\n",
            "('id','epoch','latitude','longitude')\n",
            "('5eed7714096e590006985648', '2022-05-26 12:17:16', 51.125722079524266, 149.29128965731803)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "64gQdpYSO9PF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}